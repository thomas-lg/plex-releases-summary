"""Main application entrypoint: orchestrates Tautulli fetching and Discord notification."""

import importlib.metadata
import logging
import os
import re
import sys
import time
from datetime import UTC, datetime, timedelta

import requests

from config import DEFAULT_CONFIG_PATH, Config, get_bootstrap_log_level, load_config
from discord_client import DiscordMediaItem, DiscordNotifier
from logging_config import setup_logging
from scheduler import run_scheduled
from tautulli_client import (
    TautulliClient,
    TautulliMediaItem,
    TautulliRecentlyAddedPayload,
    TautulliServerIdentity,
)

logger = logging.getLogger("app")

# Constants
DEFAULT_INFO_DISPLAY_LIMIT = 10  # Number of items to display in INFO log level
MAX_FETCH_ITERATIONS = 50
MAX_FETCH_COUNT = 10000


def _get_config_path() -> str:
    """Resolve config file path from env var override or default container path."""
    return os.getenv("CONFIG_PATH", DEFAULT_CONFIG_PATH)


def _calculate_batch_params(days: int, override: int | None = None) -> tuple[int, int]:
    """
    Calculate initial batch size and increment based on time range.

    Args:
        days: Number of days to look back
        override: Optional override value from environment variable

    Returns:
        Tuple of (initial_count, increment)
    """
    if override is not None:
        return (override, override)

    if days <= 7:
        return (100, 100)
    elif days <= 30:
        return (200, 200)
    else:
        return (500, 500)


def _format_display_title(item: TautulliMediaItem) -> str:
    """
    Format display title based on media type.

    Args:
        item: Media item dictionary from Tautulli

    Returns:
        Formatted display title string
    """
    media_type = item.get("media_type", "unknown")

    if media_type == "episode":
        show = item.get("grandparent_title", "Unknown Show")
        season_num = item.get("parent_media_index", "?")
        episode_num = item.get("media_index", "?")
        episode_title = item.get("title", "Unknown Episode")
        # Format episode numbers safely, handling non-integer values
        try:
            s_num = int(season_num) if season_num != "?" else 0
            e_num = int(episode_num) if episode_num != "?" else 0
            return f"{show} - S{s_num:02d}E{e_num:02d} - {episode_title}"
        except (ValueError, TypeError):  # fmt: skip
            return f"{show} - S{season_num}E{episode_num} - {episode_title}"
    elif media_type == "season":
        show = item.get("parent_title", "Unknown Show")
        season_num = item.get("media_index", "?")
        return f"{show} - Season {season_num}"
    elif media_type == "show":
        show = str(item.get("title", "Unknown Show"))
        year = item.get("year", "")
        return f"{show} ({year})" if year else f"{show} (New Series)"
    elif media_type == "track":
        artist = item.get("grandparent_title", "Unknown Artist")
        album = item.get("parent_title", "Unknown Album")
        track = item.get("title", "Unknown Track")
        return f"{artist} - {album} - {track}"
    elif media_type == "album":
        artist = item.get("parent_title", "Unknown Artist")
        album = item.get("title", "Unknown Album")
        return f"{artist} - {album}"
    elif media_type == "movie":
        title = str(item.get("title", "Unknown Movie"))
        year = item.get("year", "")
        return f"{title} ({year})" if year else title
    else:
        title = item.get("title", "Unknown")
        return str(title)


def _fetch_items(
    tautulli: TautulliClient,
    days: int,
    initial_batch_size: int | None = None,
) -> list[TautulliMediaItem]:
    """
    Fetch recently added items from Tautulli, iterating until all items in the
    date range are retrieved or a guardrail limit is hit, then filter by date.

    Note: Tautulli lacks server-side date filtering, so batches are expanded
    progressively and filtered client-side against the cutoff timestamp.

    Args:
        tautulli: Tautulli API client
        days: Number of days to look back
        initial_batch_size: Optional override for the initial batch size

    Returns:
        List of media items added within the last ``days`` days

    Raises:
        requests.RequestException: On network failures
        ValueError: On invalid API responses
        RuntimeError: On unexpected Tautulli errors
    """
    cutoff_timestamp = int((datetime.now(UTC) - timedelta(days=days)).timestamp())
    logger.debug("Filtering items to show only those added after timestamp: %d", cutoff_timestamp)

    initial_count, increment = _calculate_batch_params(days, override=initial_batch_size)
    current_count = initial_count
    iteration = 0
    items: list[TautulliMediaItem] = []

    while True:
        iteration += 1

        if iteration > MAX_FETCH_ITERATIONS:
            logger.warning(
                "Reached max fetch iterations (%d); proceeding with latest batch and date filtering",
                MAX_FETCH_ITERATIONS,
            )
            break

        logger.debug("Iteration %d: Fetching batch with count=%d", iteration, current_count)

        # Small delay between iterations to avoid hammering the API
        if iteration > 1:
            time.sleep(0.2)

        items_raw: TautulliRecentlyAddedPayload = tautulli.get_recently_added(days=days, count=current_count)

        # Handle both dict (newer API) and list (older API) response formats
        if isinstance(items_raw, dict) and "recently_added" in items_raw:
            items = items_raw["recently_added"]
        elif isinstance(items_raw, list):
            items = items_raw
        else:
            items = []

        if not items:
            logger.debug("No items returned, stopping iteration")
            break

        # If we received fewer items than requested, we've hit the API's limit
        if len(items) < current_count:
            logger.debug("Received %d items (less than requested %d), reached API limit", len(items), current_count)
            break

        oldest_timestamp = int(items[-1].get("added_at", 0))

        if oldest_timestamp >= cutoff_timestamp:
            # Oldest item is still in range â€” expand the batch
            next_count = current_count + increment
            if next_count > MAX_FETCH_COUNT:
                logger.warning(
                    "Reached max fetch count limit (%d); proceeding with current results",
                    MAX_FETCH_COUNT,
                )
                break
            logger.debug(
                "Oldest item still in range (iteration %d), fetching more items (next count: %d)",
                iteration,
                next_count,
            )
            current_count = next_count
        else:
            # Oldest item is outside the range â€” we have everything we need
            logger.debug("Fetched beyond time range after %d iteration(s)", iteration)
            break

    # Client-side date filter
    items_before_filter = len(items)
    items = [item for item in items if int(item.get("added_at", 0)) >= cutoff_timestamp]

    if iteration > 1:
        logger.info(
            "Retrieved %d items in %d iterations, filtered to %d items from last %d days",
            items_before_filter,
            iteration,
            len(items),
            days,
        )
    else:
        logger.info("Retrieved %d items, filtered to %d items from last %d days", items_before_filter, len(items), days)

    return items


def _build_discord_payload(items: list[TautulliMediaItem]) -> list[DiscordMediaItem]:
    """
    Build the Discord media payload from Tautulli items and log each entry.

    Logs up to DEFAULT_INFO_DISPLAY_LIMIT items per media type at INFO level;
    excess items are counted and reported in a single summary line.

    Args:
        items: Filtered list of Tautulli media items

    Returns:
        List of DiscordMediaItem dicts ready for the notifier
    """
    discord_items: list[DiscordMediaItem] = []
    suppressed_by_type: dict[str, int] = {}
    displayed_by_type: dict[str, int] = {}
    debug_enabled = logger.isEnabledFor(logging.DEBUG)

    for item in items:
        added_at = int(item.get("added_at", 0))
        date_str = datetime.fromtimestamp(added_at, tz=UTC).strftime("%Y-%m-%d %H:%M")
        date_str_short = datetime.fromtimestamp(added_at, tz=UTC).strftime("%Y-%m-%d")
        media_type = item.get("media_type", "unknown")
        display_title = _format_display_title(item)

        if debug_enabled:
            logger.debug("âž• %s | added: %s", display_title, date_str)
        else:
            shown_count = displayed_by_type.get(media_type, 0)
            if shown_count < DEFAULT_INFO_DISPLAY_LIMIT:
                logger.info("âž• %s | added: %s", display_title, date_str)
                displayed_by_type[media_type] = shown_count + 1
            else:
                suppressed_by_type[media_type] = suppressed_by_type.get(media_type, 0) + 1

        discord_item: DiscordMediaItem = {
            "type": media_type,
            "title": display_title,
            "added_at": date_str_short,
        }
        rating_key = item.get("rating_key")
        if rating_key is not None:
            discord_item["rating_key"] = rating_key
        discord_items.append(discord_item)

    if suppressed_by_type:
        suppressed_summary = ", ".join(
            f"{media_type}: {count}" for media_type, count in sorted(suppressed_by_type.items())
        )
        logger.info(
            "... additional items hidden at INFO level by type (%s). "
            "Set log_level: DEBUG in config.yml to see all entries.",
            suppressed_summary,
        )

    return discord_items


def _send_discord_notification(
    config: Config,
    tautulli: TautulliClient,
    discord_items: list[DiscordMediaItem],
    days: int,
    total_count: int,
) -> int:
    """
    Auto-detect Plex server ID (if needed) and dispatch the Discord summary.

    Args:
        config: Application configuration
        tautulli: Tautulli client used for server identity auto-detection
        discord_items: Payload built by _build_discord_payload
        days: Days-back value forwarded to the notifier for display
        total_count: Total item count forwarded to the notifier for display

    Returns:
        0 on success or in scheduled mode (non-fatal errors); 1 on hard failure
        in one-shot mode
    """
    logger.debug("Discord webhook URL configured, sending notification...")
    try:
        plex_server_id = config.plex_server_id

        # Auto-fetch Plex Server ID from Tautulli if not provided
        if not plex_server_id:
            logger.debug("plex_server_id not configured, fetching from Tautulli...")
            try:
                server_info: TautulliServerIdentity = tautulli.get_server_identity()
                plex_server_id = server_info.get("machine_identifier")
                if plex_server_id:
                    logger.info("Auto-detected Plex Server ID: %s", plex_server_id)
                else:
                    logger.warning("Could not auto-detect Plex Server ID from Tautulli")
            except requests.RequestException as e:
                logger.warning("Network error while fetching Plex Server ID: %s", e)
            except (ValueError, RuntimeError) as e:
                logger.warning("Invalid response from Tautulli: %s", e)

        webhook_url = config.discord_webhook_url
        if webhook_url is None:
            raise RuntimeError("discord_webhook_url must not be None â€” ensure it is set in config.yml")

        notifier = DiscordNotifier(webhook_url, config.plex_url, plex_server_id)
        success = notifier.send_summary(discord_items, days, total_count)
        if not success and config.run_once:
            return 1
    except requests.RequestException as e:
        logger.error("Network error while sending Discord notification: %s", e)
        if config.run_once:
            return 1
    except ValueError as e:
        logger.error("Invalid Discord webhook configuration: %s", e)
        if config.run_once:
            return 1
    except Exception as e:
        logger.exception("Unexpected error while sending Discord notification: %s", e)
        if config.run_once:
            return 1
    return 0


def run_summary(config: Config) -> int:
    """
    Execute the Plex summary task: fetch and display recently added media.

    Args:
        config: Application configuration

    Returns:
        Exit code: 0 for success, 1 for error
    """
    logger.info("ðŸš€ Starting Plex summary (last %d days)", config.days_back)

    tautulli = TautulliClient(base_url=config.tautulli_url, api_key=config.tautulli_api_key)

    logger.info("Querying recently added items with iterative fetching...")
    try:
        items = _fetch_items(tautulli, config.days_back, config.initial_batch_size)
    except requests.RequestException as e:
        logger.error("Network error while fetching recently added items: %s", e)
        return 1
    except ValueError as e:
        logger.error("Invalid response from Tautulli API: %s", e)
        return 1
    except Exception as e:
        logger.exception("Unexpected error while fetching recently added items: %s", e)
        return 1

    discord_items = _build_discord_payload(items)

    if config.discord_webhook_url:
        exit_code = _send_discord_notification(config, tautulli, discord_items, config.days_back, len(items))
    else:
        logger.debug("No Discord webhook URL configured, skipping Discord notification")
        exit_code = 0

    logger.info("âœ… Run complete: %d items in the last %d days", len(items), config.days_back)
    return exit_code


def main():
    """
    Main entry point: Choose between scheduled or one-shot execution mode.

    Execution mode is configured via config.yml:
      - set run_once: true to run once and exit
      - or provide cron_schedule to run as a persistent scheduled task
    """
    config_path = _get_config_path()

    # Bootstrap logging level from raw config so load-time logs honor user verbosity
    setup_logging(get_bootstrap_log_level(config_path))

    version = os.getenv("APP_VERSION") or None
    if not version:
        try:
            version = importlib.metadata.version("plex-releases-summary")
        except importlib.metadata.PackageNotFoundError:
            version = "unknown"

    version_display = f"v{version}" if re.match(r"^\d+\.\d+\.\d+", version) else version

    print(rf"""
  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
   ____  ____  ____
  |  _ \|  _ \/ ___|
  | |_) | |_) \___ \
  |  __/|  _ < ___) |
  |_|   |_| \_\____/

  Plex Releases Summary  Â·  {version_display}
  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
""")
    logger.info("Starting Plex Releases Summary %s", version_display)

    try:
        config = load_config(config_path)
    except Exception as e:
        logger.exception("FATAL: Failed to load configuration: %s", e)
        return 1

    # Now setup logging with config
    setup_logging(config.log_level)

    if config.run_once:
        # One-shot mode: run once and exit
        logger.info("â–¶ï¸  Starting in ONE-SHOT mode (run_once=true)")
        return run_summary(config)
    else:
        # Scheduled mode: run as daemon with CRON schedule
        logger.info("ðŸ“… Starting in SCHEDULED mode")
        # Guaranteed non-None by Pydantic model validator (validate_cron_schedule_required)
        if config.cron_schedule is None:  # pragma: no cover
            raise RuntimeError("cron_schedule must not be None when run_once is False")
        # Wrap run_summary to pass config
        return run_scheduled(lambda: run_summary(config), config.cron_schedule)


if __name__ == "__main__":
    try:
        exit_code = main()
        sys.exit(exit_code)
    except KeyboardInterrupt:
        logger.info("Interrupted by user")
        sys.exit(130)
    except Exception as e:
        logger.exception("Unexpected error: %s", e)
        sys.exit(1)
